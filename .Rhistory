axis(2,labels=FALSE,tcl=-0.35)  ; axis(2, lwd=0, line=-0.4, las=2, labels=TRUE)
axis(3,labels=FALSE,tcl=-0.35) ; axis(4,labels=FALSE,tcl=-0.25)
box()
axis(1,labels=FALSE, tcl=-0.35) ; axis(1, lwd=0, line=-0.4, labels=TRUE)
axis(2,labels=FALSE,tcl=-0.35)  ; axis(2, lwd=0, line=-0.4, las=2, labels=TRUE)
axis(3,labels=FALSE,tcl=-0.35) ; axis(4,labels=FALSE,tcl=-0.25)
mtcars
length(mtcars)
str(mtcars)
summary(mtcars)
barplot(mtcars$mpg, names.arg = row.names(mtcars))
pairs(mtcars)
pairs(mtcars, lower.panel = NULL)
plot(y = ,mtcars$mpg, x= mtcars$wt)
pairs(mtcars, lower.panel = NULL)
plot(y = ,mtcars$mpg, x= mtcars$disp)
pairs(mtcars, lower.panel = NULL)
plot(y = ,mtcars$mpg, x= mtcars$hp)
pairs(mtcars, lower.panel = NULL)
formula <-  matcars$mpg ~ mtcars$disp   # it says to express mpg as function of disp
model <- lm(formula)    # fits a linear regression to the formula
formula <-  mtcars$mpg ~ mtcars$disp   # it says to express mpg as function of disp
model <- lm(formula)    # fits a linear regression to the formula
coef(model)
plot(y = ,mtcars$mpg, x= mtcars$disp)
abline(a = coef(model)[1], b = coef(model)[2])
form2 <- mtcars$mpg ~ mtcars$wt
model2 <- lm(form2)
plot(y = ,mtcars$mpg, x= mtcars$wt)
abline(model2)
pairs(mtcars, lower.panel = NULL)
coef(model2)
full_model <- lm(fit)
fit <- mtcars$mpg ~ mtcars$wt + mtcars$disp + mtcars$hp + mtcars$drat
full_model <- lm(fit)
### put everything together
fit <- lm(mtcars$mpg ~ mtcars$wt + mtcars$disp + mtcars$hp + mtcars$drat)
plot(fit)
coef(fit)
summary(fit)
vcov(fit)
summary(model2)
setwd('~/Desktop/dataSaoPaulo2018/')
adv <- read.csv('Advertising.csv')
head(adv)
adv <- read.csv('Advertising.csv' )
head(adv)
adv <- read.csv('Advertising.csv' )
head(adv)
adv.lm <- lm(sales ~ TV)
adv.lm <- lm(adv$sales ~ adv$TV)
summary(adv.lm)
adv.lm2 <- lm(adv$sales ~ adv$newspaper)
summary(adv.lm2)
plot(y = adv$sales, x= adv$newspaper)
abline(adv.lm2)
adv.lm3 <- lm(adv$sales ~ adv$TV + adv$radio + adv$newspaper)
summary(adv.lm3)
adv.lm4 <- lm(adv$sales ~ adv$TV + adv$radio)
summary(adv.lm3)
#######################################
library(cars)
#######################################
library(car)
df <- Prestige
head(df)
sim(df)
summary(df)
length(df)
df
prestige.fit <- lm(df$prestige ~ df$education + df$income + df$women + df$census)
summary(prestige.fit)
prestige.fit <- lm(df$prestige ~ df$education + df$income + df$women)
summary(prestige.fit)
prestige.fit <- lm(df$prestige ~ df$education + df$income + df$women + df$census)
summary(prestige.fit)
pairs(df)
pairs(df, lower.panel = NULL)
pairs(df)
prestige.fit <- lm(df$prestige ~ df$education + df$income)
summary(prestige.fit)
prestige.fit <- lm(df$prestige ~ df$education)
prestige.fit <- lm(df$prestige ~ df$education)
summary(prestige.fit)
plot(y = df$prestige, x= df$education )
abline(prestige.fit)
prestige.fit <- lm(df$prestige ~  df$income)
plot(y = df$prestige, x= df$income )
abline(prestige.fit)
prestige.fit <- lm(df$prestige ~  df$education)
summary(prestige.fit)
plot(y = df$prestige, x= df$education )
abline(prestige.fit)
prestige.fit <- lm(df$prestige ~  df$education + df$income )
plot(prestige.fit)
prestige.edfit <- lm(df$prestige ~  df$education )
prestige.incfit <- lm(df$prestige ~ df$income)
prestige.fit <- lm(df$prestige ~  df$education + df$income )
coef(prestige.edfit)
coef(prestige.incfit)
coef(prestige.fit)
summary(prestige.edfit)
summary(prestige.incfit)
summary(prestige.fit)
coef(prestige.edfit)
coef(prestige.incfit) # <---- from here you can see that income is not that important
coef(prestige.fit)
pop <- data.frame(uspop)
head(pop)
# Dataset is a little bit confusing, let's deal with that
length(pop)
# Dataset is a little bit confusing, let's deal with that
dim(pop)
pop$year <- seq(from = 1970, to = 1970. by =10)
pop$year <- seq(from = 1970, to = 1970, by =10)
head(pop)
unique(pop$year )
pop$year <- seq(from = 1790, to = 1970, by =10)
head(pop)
unique(pop$year)
plot(x = pop$year, y = pop$uspop)
linpop <- lm(pop$uspop ~ pop$year)
plot(x = pop$year, y = pop$uspop)
abline(linpop)
coef(polypop)
polypop <- lm(pop$uspop ~ poly(pop$year,2))
plot(residuals(linpop), x=fitted(linpop))
plot(x = pop$year, y = pop$uspop)
lines(sort(pop$year), fitted(polypop), col = 'blue', lty=2)
## way better, let's see the residuals
plot(residuals(polypop), x=fitted(polypop)) ### good way to see that the linear model doesn't fit well
plot(y = mtcars$mpg, x = mtcars$wt)
cars.log <- glm(am ~ mtcars$mpg + mtcars$wt, family = 'binomial')
cars.log <- glm(mtcars$am ~ mtcars$mpg + mtcars$wt, family = 'binomial')
summary(cars.log)
cars.log
a = coef(cars.log[1])
b = coef(cars.log[2])
c = coef(cars.log[3])
int.cars <- -(a/c)
slope.cars <- -(b/c)
plot(wt ~ mpg, data = mtcars, pch = am)
plot(wt ~ mpg, data = mtcars, pch = am)
abline(a=int.cars, b= slope.cars)
int.cars <- -(a/c)
slope.cars <- -(b/c)
int.cars
coefs(cars.log)
coef(cars.log)
coef(cars.log)[1]
a = coef(cars.log)[1]
b = coef(cars.log)[2]
c = coef(cars.log)[3]
int.cars <- -(a/c)
slope.cars <- -(b/c)
int.cars
plot(wt ~ mpg, data = mtcars, pch = am)
abline(a=int.cars, b= slope.cars)
library(party)
tree <- ctree(mpg ~ . , data = mtcars)
plot(tree)
ally
### significant variables for mpg, At node 1, there is a split for cars that weigh less
### than 2.32 tons and those that weigh more. For the cars that weigh more, we split
### further on the engine displacement. For engine displacements that are less than 258
### cubic inches in volume, we go to node 4. For engine displacements that have more
### than 258 cubic inches, we go to node 5. Notice that for each feature there is a
### statistical p-value, which determines how statis‐ tically relevant it is.
### The closer the p-value is to 0.05 or greater, the less useful or rele‐ vant it is.
head(iris)
iris.tree <- ctree(Species ~ . , data=iris)
plot(iris.tree)
iris.tree <- ctree(Petal.Length ~ . , data=iris)
plot(iris.tree)
iris.tree <- ctree(Species ~ . , data=iris)
plot(iris.tree)
unique(iris$Species)
library(e1071)
s <- sample(150, 100)
iris_train <- iris[s, col]
iris_test <- iris[-s, col]
c <- c("Petal_Length", "Petal_Width", "Species")
iris_train <- iris[s, col]
iris_test <- iris[-s, col]
col <- c("Petal_Length", "Petal_Width", "Species")
iris_train <- iris[s, col]
iris_test <- iris[-s, col]
##############################################
###### Support Vector Machines  ##############
##############################################
### To find dividing hypersurfaces in data
iris
col <- c("Petal.Length", "Petal.Width", "Species")
iris_train <- iris[s, col]
iris_test <- iris[-s, col]
summary(iris_train)
svmfit <- svm(Species ~ ., data = iris_train, kernel= 'linear')
plot(svmfit, iris_train[, col])
plot(Petal.Length, Petal.Width , data=iris, color= Species)
plot(iris$Petal.Length, iris$Petal.Width , color= iris$Species)
plot(iris$Petal.Length, iris$Petal.Width , col= iris$Species)
plot(svmfit, iris_train[, col])
svmfit <- svm(Species ~ ., data = iris_test, kernel= 'linear')
plot(svmfit, iris_train[, col])
svmfit <- svm(Species ~ ., data = iris_train, kernel= 'linear')
plot(svmfit, iris_test[, col])
plot(svmfit, iris_train[, col])
iris.new <- iris[,c(1,2,3,4)]
iris.new
head(iris.new)
?kmeans
iris.kmeans <- kmeans(iris.new, centers = 3)
plot(iris.new[,c(1,2)], col = iris.kmeans$cluster)
#########################################################
##################   Challenge  #########################
#########################################################
df <- read.csv('event-0005.csv')
head(df)
unique(df$vjet)
unique(jet)
unique(df$jet)
train_df <- df[,-1]
head(train_df)
unique(df$constituent)
dim(train_df)
train_df <- df[,-c(1,8)]
head(train_df)
head(df)
train_df <- df[,-c(1,9)]
head(train_df)
unique(df$jet)
unique(df$constituent)
train_df <- df[,-c(1,2,9)]
head(train_df)
jet.kmeans <- kmeans(train_df, centers = 3)
plot(train_df[,c(1,2)], col = jet.kmeans$cluster)
pairs(df)
pairs(df[-9])
train_df <- df[,-c(1,2,8,9)]
jet.kmeans <- kmeans(train_df, centers = 3)
plot(train_df[,c(1,2)], col = jet.kmeans$cluster)
pairs(jet.kmeans)
pairs(train_df)
plot(train_df[,c(1,2)], col = jet.kmeans$cluster)
plot(train_df, col = jet.kmeans$cluster)
plot(df[3:7], col = df$jet)
plot(df[3:7], col = df$jet +1)
summary(jet.kmeans)
table(df$jet, jet.kmeans$cluster)
table(df$jet+1, jet.kmeans$cluster)
cm <- table(df$jet+1, jet.kmeans$cluster)
cm <- table(df$jet, jet.kmeans$cluster)
cm
plot(train_df, col = jet.kmeans$cluster)
plot(df[3:7], col = df$jet +1)
cm <- table(df$jet, -jet.kmeans$cluster)
cm
?ifelse
jet.kmeans$cluster
table(jet.kmeans$cluster)
table(df$jet)
cm <- table(df$jet, -jet.kmeans$cluster)
cm
table(df$jet)
table(jet.kmeans$cluster)
table(-jet.kmeans$cluster)
jet.svm <- svm(jet ~ ., data=df, kernel = 'linear')
jet.svm <- svm(jet ~ ., data=df[-9], kernel = 'linear')
plot(jet.svm, df[2,3])
plot(jet.svm)
plot(jet.svm)
jet.svm
plot(jet.svm, df[2:3])
plot(jet.svm, df[,2:3])
plot(jet.svm, df[,2])
plot(train_df, col = jet.kmeans$cluster)
jet.svm <- svm(jet ~ ., data=df[-9], kernel = 'linear')
jet.svm <- svm(jet ~ ., data=df[-9], kernel = 'linear')
plot(jet.svm, df[,2])
summary(jet.svm)
jet.svm <- svm(jet ~ ., data=df[-9], kernel= 'polynomial')
plot(jet.svm, df[,2])
summary(jet.svm)
jet.svm <- svm(jet ~ ., data=df[-9], kernel= 'sigmoid')
plot(jet.svm, df[,2])
summary(jet.svm)
head(train_df)
cols <- c('eta','phi','px','py', 'pz')
plot(jet.svm, df[,cols])
summary(jet.svm)
jet.svm <- svm(jet ~ ., data=train_df[,cols], kernel= 'sigmoid')
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'sigmoid')
#### SVM
cols <- c('jet','eta','phi','px','py', 'pz')
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'sigmoid')
plot(jet.svm, df[,cols])
summary(jet.svm)
plot(jet.svm, df[,cols])
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'sigmoid')
plot(jet.svm, df[,cols])
summary(jet.svm)
## https://www.statmethods.net/advstats/cluster.html
aggregate(train_df, by = list(jet.kmeans$cluster), FUN = mean)
train_df
new.df <- data.frame(train_df, jet.kmeans$cluster)
table(new.df$jet.kmeans.cluster)
table(df)
table(df$jet)
table(-new.df$jet.kmeans.cluster)
library(cluster)
clusplot(new.df , jet.kmeans$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
##################################
########## Challenge #############
##################################
setwd('~/Desktop/codata2018_git/')
df <- read.csv('event-0005.csv')
head(df)
setwd('~/Desktop/codata2018_git/')
pairs(df[-9])
# Data exploration
pairs(df[-9])
train_df <- df[,-c(1,2,8,9)]
head(train_df)
pairs(train_df)
jet.kmeans <- kmeans(train_df, centers = 3)
plot(train_df, col = jet.kmeans$cluster)
summary(jet.kmeans)
aggregate(train_df,by=list(jet.kmeans$cluster),FUN=mean)
library(cluster)
clusplot(train_df, jet.kmeans$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
plot(train_df, col = jet.kmeans$cluster)
table(-jet.kmeans$cluster)
table(df$jet)
table(df$jet, -jet.kmeans$cluster)
# compare the counting with the original dataset (cheating)
table(jet.kmeans$cluster)
table(df$jet)
# compare the counting with the original dataset (cheating)
table(jet.kmeans$cluster +1)
# compare the counting with the original dataset (cheating)
table(jet.kmeans$cluster)
table(df$jet)
jet.kmeans <- kmeans(train_df, centers = 3)
table(jet.kmeans$cluster)
##################################
########## Challenge #############
##################################
setwd('~/Desktop/codata2018_git/')
df <- read.csv('event-0005.csv')
train_df <- df[,-c(1,2,8,9)]
head(train_df)
pairs(train_df)
jet.kmeans <- kmeans(train_df, centers = 3)
plot(train_df, col = jet.kmeans$cluster)
table(jet.kmeans$cluster)
table(df$jet)
table(jet.kmeans$cluster+1)
# the confusion matrix
table(df$jet, -jet.kmeans$cluster)
jet.tree <- ctree(df$jet ~ .)
jet.tree <- ctree(df$jet ~ ., data= df)
plot(jet.tree)
cols <- c('jet','eta','phi','px','py', 'pz')
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'sigmoid')
plot(jet.svm, df[,cols])
plot(jet.svm, jet, df[,cols])
plot(jet.svm, df$jet, df[,cols])
summary(jet.svm)
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'linear')
plot(jet.svm, df$jet, df[,cols])
summary(jet.svm)
plot(jet.svm, df[,cols])
library(e1071)
cols <- c('jet','eta','phi','px','py', 'pz')
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'linear')
plot(jet.svm, df[,cols])
print(jet.svm)
print(jet.svm)
plot(jet.svm)
jet.svm$index
jet.svm$nclasses
plot(jet.svm, df[, cols])
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'radial')
print(jet.svm)
plot(jet.svm, df[, cols])
jet.svm$nclasses
cols <- c('eta','phi','px','py', 'pz')
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'radial')
plot(jet.svm, df[, cols])
cols <- c('jet','eta','phi','px','py', 'pz')
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'radial')
plot(jet.svm, df[, cols])
print(jet.svm)
plot(jet.svm, df$eta])
plot(jet.svm, df$eta)
plot(jet.svm, def$jet, df$eta)
plot(jet.svm, df$jet, df$eta)
jet.svm <- svm(factor(jet) ~ ., data=df[,cols], kernel= 'radial')
plot(jet.svm, df$jet, df$eta)
s <- sample(dim(df), 150)
jet_train <- df[s, col]
jet_test <- df[-s, col]
s <- sample(dim(df), 150)
#### SVM
s <- sample(dim(df), 100)
dim(df)
#### SVM
s <- sample(205, 150)
jet_train <- df[s, col]
jet_train <- df[s, cols]
jet_test <- df[-s, cols]
jet_train$jet <- factor(jet_train$jet)
jet.svm <- svm(factor(jet) ~ ., data=df[,cols], kernel= 'radial')
plot(jet.svm, df$jet, df$eta)
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'radial')
plot(jet.svm, df$jet, df$eta)
plot(jet.svm, jet_train[, cols])
plot(jet.svm, jet_train[, cols])
plot(jet.svm, jet_train)
prediction <- predict(jet.svm, jet_test)
table(prediction)
jet.svm <- svm(jet ~ ., data=df[,cols], kernel= 'linear')
plot(jet.svm, jet_train)
prediction <- predict(jet.svm, jet_test)
table(prediction)
jet.svm <- svm(jet ~ ., data=jet_train, kernel= 'linear')
plot(jet.svm, jet_train)
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train)
plot(jet.svm, jet_train[,cols])
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train[,cols])
prediction <- predict(jet.svm, jet_test)
table(prediction)
plot(jet.svm, jet_train[,cols])
plot(jet.svm, jet_train[,cols])
plot(prediction)
table(prediction)
table(prediction, jet_test$jet)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$phi )
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
jet_train$jet <- factor(jet_train$jet)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet + jet_train$eta)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
plot(df[3:7], col = df$jet +1)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
jet_train <- df[s, cols]
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
plot(jet.svm, jet_train[,cols])
plot(jet.svm, jet_train)
cols = c('jet', 'eta', 'phi')
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train)
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
prediction <- predict(jet.svm, jet_test)
table(prediction, jet_test$jet)
cols = c('jet', 'phi', 'pz')
jet_train$jet <- factor(jet_train$jet)
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
jet_train$jet <- factor(jet_train$jet)
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
cols = c('jet', 'phi', ,'eta','pz')
cols = c('jet', 'phi', 'eta','pz')
jet_train <- df[s, cols]
jet_test <- df[-s, cols]
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
jet_train$jet <- factor(jet_train$jet)
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
jet_train$jet <- factor(jet_train$jet)
jet.svm <- svm(jet ~ ., data=jet_train[,cols], kernel= 'linear')
plot(jet.svm, jet_train, formula = jet_train$jet ~ jet_train$eta)
plot(jet.svm, jet_train[,cols], formula = jet_train$jet ~ jet_train$eta)
plot(jet.svm, jet_train[,cols])
plot(jet.svm, jet_train, jet ~ eta)
jet.svm <- svm(jet ~ ., data=jet_train, kernel= 'linear')
plot(jet.svm, jet_train, jet ~ eta)
plot(jet.svm, jet_train, phi ~ eta)
plot(jet.svm, jet_train, jet ~ eta)
plot(jet.svm, jet_train, eta ~ phi)
plot.svm(jet.svm, jet_train)
plot(jet.svm, jet_train, eta ~ phi)
#cols = c('jet', 'phi', 'eta','pz')
cols <- c('jet','eta','phi','px','py', 'pz')
jet_train$jet <- factor(jet_train$jet)
jet.svm <- svm(jet ~ ., data=jet_train, kernel= 'linear')
plot(jet.svm, jet_train, eta ~ px)
jet_train <- df[s, cols]
jet_test <- df[-s, cols]
jet.svm <- svm(jet ~ ., data=jet_train, kernel= 'linear')
plot(jet.svm, jet_train, eta ~ px)
plot(jet.svm, jet_train, eta ~ px)
plot(jet.svm, jet_train, eta ~ phi)
jet_train <- df[s, cols]
jet.svm <- svm(jet ~ ., data=jet_train, kernel= 'linear')
plot(jet.svm, jet_train, eta ~ phi)
cols = c('jet', 'phi', 'eta','pz')
jet_train <- df[s, cols]
jet_test <- df[-s, cols]
jet.svm <- svm(jet ~ ., data=jet_train, kernel= 'linear')
plot(jet.svm, jet_train, eta ~ phi)
plot(jet.svm, jet_train, eta ~ px)
setwd('~/Desktop/codata2018_git/')
library(neuralnet)
